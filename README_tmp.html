<!DOCTYPE html>
<html>

<head>
    <title>README.md</title>
    <meta http-equiv="Content-type" content="text/html;charset=UTF-8">
    
<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

html,footer,header{
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Custom MD PDF CSS
 */
html,footer,header{
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";

 }
body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>
<link rel="stylesheet" href="file:///Users/simonabadi/Desktop/Machine%20Learning/summary-ml-2025/R%3A%5C2.Travail%5C1.Enseignement%5CCours%5C_1.Outils%5C2.Developpement%5C1.SCSS%5Cmain.css" type="text/css"><link rel="stylesheet" href="file:///Users/simonabadi/Desktop/Machine%20Learning/summary-ml-2025/D%3A%5Crdaros%5CCours%5C_1.Outils%5C2.Developpement%5C1.SCSS%5Cmain.css" type="text/css">
</head>

<body>
    <h1 id="%F0%9F%A4%96-machine-learning-from-data---course-summary">🤖 Machine Learning from Data - Course Summary</h1>
<div align="center">
</div><p><img src="https://img.shields.io/badge/Machine%20Learning-CS3141-blue?style=for-the-badge" alt="Machine Learning"><br>
<img src="https://img.shields.io/badge/Reichman%20University-2025-orange?style=for-the-badge" alt="University"><br>
<img src="https://img.shields.io/badge/Status-Complete-green?style=for-the-badge" alt="Status"></p>
<p><em>A comprehensive overview of machine learning concepts covering supervised learning, unsupervised learning, and optimization techniques.</em></p>
<p><strong>Instructors:</strong> Prof. Ilan Gronau &amp; Dr. Alon Kipnis</p>

<hr>
<h2 id="%F0%9F%93%9A-table-of-contents">📚 Table of Contents</h2>
<ul>
<li><a href="#-supervised-learning">🎯 Supervised Learning</a>
<ul>
<li><a href="#-k-nearest-neighbors-k-nn">🔍 K-Nearest Neighbors</a></li>
<li><a href="#-polynomial-regression">📈 Polynomial Regression</a></li>
<li><a href="#-decision-trees">🌳 Decision Trees</a></li>
<li><a href="#-bayesian-classification">🎲 Bayesian Classification</a></li>
<li><a href="#-perceptron">🧠 Perceptron</a></li>
<li><a href="#-logistic-regression">📊 Logistic Regression</a></li>
<li><a href="#-support-vector-machines-svm">⚡ Support Vector Machines</a></li>
<li><a href="#-model-evaluation">📏 Model Evaluation</a></li>
</ul>
</li>
<li><a href="#-unsupervised-learning">🔍 Unsupervised Learning</a>
<ul>
<li><a href="#-data-distribution-learning">📊 Data Distribution Learning</a></li>
<li><a href="#-clustering">🔗 Clustering</a></li>
<li><a href="#-expectation-maximization">🔄 Expectation Maximization</a></li>
<li><a href="#-dimensionality-reduction">📉 Dimensionality Reduction</a></li>
</ul>
</li>
<li><a href="#%EF%B8%8F-optimization">⚙️ Optimization</a>
<ul>
<li><a href="#-gradient-based-methods">🎯 Gradient-Based Methods</a></li>
<li><a href="#-loss-functions">📉 Loss Functions</a></li>
<li><a href="#-constrained-optimization">🔒 Constrained Optimization</a></li>
</ul>
</li>
</ul>
<hr>
<h2 id="%F0%9F%8E%AF-supervised-learning">🎯 Supervised Learning</h2>
<p>Supervised learning involves training models on labeled data to make predictions on new, unseen data. The training process uses input-output pairs to learn patterns and relationships.</p>
<h3 id="%F0%9F%94%8D-k-nearest-neighbors-k-nn">🔍 K-Nearest Neighbors (K-NN)</h3>
<p><strong>Core Concept:</strong> Make predictions based on the k nearest neighbors in the feature space.</p>
<ul>
<li><strong>For Regression:</strong> Average the values of k nearest neighbors</li>
<li><strong>For Classification:</strong> Take majority vote among k nearest neighbors</li>
<li><strong>Hyperparameter:</strong> k (number of neighbors)</li>
</ul>
<p><strong>Pros:</strong></p>
<ul>
<li>✅ Good fit to training data with minimal assumptions</li>
<li>✅ Works well with non-linear patterns</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>❌ Computationally inefficient for large datasets</li>
<li>❌ Struggles with high-dimensional spaces (curse of dimensionality)</li>
</ul>
<h3 id="%F0%9F%93%88-polynomial-regression">📈 Polynomial Regression</h3>
<p><strong>Core Concept:</strong> Fit polynomial functions to data by transforming features into higher-degree terms.</p>
<ul>
<li><strong>Process:</strong> Transform features using polynomial basis functions → Solve linear regression</li>
<li><strong>Hyperparameter:</strong> k (maximum degree of polynomial)</li>
<li><strong>Method:</strong> Minimize sum-of-squares loss via gradient descent or pseudo-inverse</li>
</ul>
<p><strong>Use Case:</strong> When relationships between variables are non-linear but can be captured by polynomial terms.</p>
<h3 id="%F0%9F%8C%B3-decision-trees">🌳 Decision Trees</h3>
<p><strong>Core Concept:</strong> Create a tree-like model of decisions to classify data points.</p>
<ul>
<li><strong>Training:</strong> Iterative node splitting using attributes that maximize impurity reduction</li>
<li><strong>Key Metrics:</strong> Entropy, Gini Impurity, Information Gain, Chi-squared test</li>
<li><strong>Hyperparameters:</strong> Max depth, max leaves, min samples per leaf, min impurity decrease</li>
</ul>
<p><strong>Pros:</strong></p>
<ul>
<li>✅ Excellent for categorical features</li>
<li>✅ No specific modeling assumptions</li>
<li>✅ Interpretable results</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>❌ Can create very large, inefficient models</li>
<li>❌ Rectangular decision boundaries only</li>
<li>❌ Prone to overfitting</li>
</ul>
<h3 id="%F0%9F%8E%B2-bayesian-classification">🎲 Bayesian Classification</h3>
<p><strong>Core Concept:</strong> Use Bayes' theorem and probability distributions to classify data.</p>
<ul>
<li><strong>Training:</strong> Fit class priors and class-conditional distributions via maximum likelihood</li>
<li><strong>Prediction:</strong> Minimize expected risk using posterior probabilities</li>
<li><strong>Variants:</strong> MAP (uniform cost), ML (uniform priors), Naïve Bayes (feature independence)</li>
</ul>
<p><strong>Formula:</strong> <code>c(x) = argmin_j Σ π_l f_{X|Y=l}(x) λ_{j,l}</code></p>
<p><strong>Pros:</strong></p>
<ul>
<li>✅ Easily scales to multiple classes</li>
<li>✅ Flexible decision boundaries</li>
<li>✅ Can incorporate different costs for classification errors</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>❌ Challenging to learn distributions in high dimensions</li>
</ul>
<h3 id="%F0%9F%A7%A0-perceptron">🧠 Perceptron</h3>
<p><strong>Core Concept:</strong> Linear classifier that finds a separating hyperplane.</p>
<ul>
<li><strong>Capability:</strong> Finds linear separation boundaries</li>
<li><strong>Extension:</strong> Non-linear boundaries via feature mapping or dual formulation with kernels</li>
<li><strong>Status:</strong> Not practical (SVM is the better alternative)</li>
</ul>
<h3 id="%F0%9F%93%8A-logistic-regression">📊 Logistic Regression</h3>
<p><strong>Core Concept:</strong> Linear classifier optimized for probabilistic interpretation.</p>
<ul>
<li><strong>Method:</strong> Minimize binary cross-entropy (BCE) loss</li>
<li><strong>Decision Boundary:</strong> Linear</li>
<li><strong>Extensions:</strong> Natural extension to multiple classes</li>
<li><strong>Hyperparameters:</strong> None (typically)</li>
</ul>
<p><strong>Best For:</strong> Cases with no pure linear separation, provides probability estimates for classifications.</p>
<h3 id="%E2%9A%A1-support-vector-machines-svm">⚡ Support Vector Machines (SVM)</h3>
<p><strong>Core Concept:</strong> Find the optimal separating hyperplane with maximum margin.</p>
<ul>
<li><strong>Objective:</strong> Maximize margin between classes</li>
<li><strong>Extensions:</strong> Kernel functions for non-linear boundaries</li>
<li><strong>Slack Variables:</strong> Allow margin violations and classification errors</li>
<li><strong>Loss Function:</strong> Hinge loss</li>
<li><strong>Hyperparameters:</strong> Kernel function, kernel parameters, C (slack variable penalty)</li>
</ul>
<p><strong>Pros:</strong></p>
<ul>
<li>✅ The &quot;go-to&quot; method for classification</li>
<li>✅ Flexible decision boundaries via kernels</li>
<li>✅ Some interpretability through support vectors</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>❌ Relatively computationally heavy to train</li>
</ul>
<h3 id="%F0%9F%93%8F-model-evaluation">📏 Model Evaluation</h3>
<p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Bias-Variance Tradeoff:</strong> Balance between underfitting and overfitting</li>
<li><strong>Data Splitting:</strong> Train → Validate → Test workflow</li>
<li><strong>Metrics:</strong> Confusion matrix, precision, recall, F1-score, ROC AUC</li>
</ul>
<p><strong>Process:</strong></p>
<ol>
<li><strong>Training Set:</strong> Fit models</li>
<li><strong>Validation Set:</strong> Select hyperparameters</li>
<li><strong>Test Set:</strong> Assess final generalization error</li>
</ol>
<hr>
<h2 id="%F0%9F%94%8D-unsupervised-learning">🔍 Unsupervised Learning</h2>
<p>Unsupervised learning finds patterns and structure in data without labeled examples, focusing on discovering hidden relationships and groupings.</p>
<h3 id="%F0%9F%93%8A-data-distribution-learning">📊 Data Distribution Learning</h3>
<p><strong>Core Concept:</strong> Learn the underlying probability distribution of data.</p>
<p><strong>Methods:</strong></p>
<ul>
<li><strong>Non-parametric:</strong> Histogram smoothing techniques</li>
<li><strong>Parametric:</strong> Maximum likelihood estimation (typically log-likelihood)</li>
</ul>
<p><strong>Common Distributions:</strong></p>
<ul>
<li>Binomial, Poisson, Exponential</li>
<li><strong>Normal Distributions:</strong> Univariate/Multivariate Gaussians (particularly useful)</li>
<li><strong>Gaussian Mixture Models (GMMs)</strong></li>
</ul>
<p><strong>Applications:</strong> Bayesian classification, understanding data properties (mean, modes, variation)</p>
<h3 id="%F0%9F%94%97-clustering">🔗 Clustering</h3>
<p><strong>Core Concept:</strong> Partition data into &quot;closely clustered&quot; subsets of samples.</p>
<p><strong>Algorithms:</strong></p>
<ul>
<li><strong>K-means:</strong> Minimize within-cluster spread for given number of clusters (k)</li>
<li><strong>Hierarchical Clustering:</strong> Build hierarchy represented by dendrogram</li>
</ul>
<p><strong>Applications:</strong></p>
<ul>
<li>Describe data structure</li>
<li>Understand generative processes (e.g., evolutionary trees)</li>
<li>Data exploration and segmentation</li>
</ul>
<h3 id="%F0%9F%94%84-expectation-maximization">🔄 Expectation Maximization</h3>
<p><strong>Core Concept:</strong> Specialized optimization for complex probabilistic models with hidden variables.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li>✅ Iterative algorithm without requiring learning rate tuning</li>
<li>✅ Useful when likelihood is simplified by introducing hidden variables</li>
<li>✅ Common for cluster labels and mixture model components</li>
</ul>
<p><strong>Applications:</strong> GMM estimation, clustering with probabilistic assignments</p>
<h3 id="%F0%9F%93%89-dimensionality-reduction">📉 Dimensionality Reduction</h3>
<p><strong>Methods:</strong> PCA (Principal Component Analysis) + LDA (Linear Discriminant Analysis)</p>
<p><em>Note: Not covered in exam for this course year</em></p>
<hr>
<h2 id="%E2%9A%99%EF%B8%8F-optimization">⚙️ Optimization</h2>
<p>Optimization techniques are fundamental to training machine learning models, involving the minimization or maximization of objective functions.</p>
<h3 id="%F0%9F%8E%AF-gradient-based-methods">🎯 Gradient-Based Methods</h3>
<p><strong>Core Concept:</strong> Use gradient information to iteratively find optimal parameters.</p>
<p><strong>Analytical Solutions:</strong></p>
<ul>
<li>Pseudoinverse method for linear regression</li>
<li>Maximum likelihood estimates for many distributions</li>
</ul>
<p><strong>Gradient Descent Variants:</strong></p>
<ul>
<li><strong>Standard GD:</strong> Full dataset gradient computation</li>
<li><strong>Stochastic GD:</strong> Use batches for efficiency</li>
<li><strong>Sub-gradient Descent:</strong> Handle non-continuous gradient points (e.g., hinge loss)</li>
</ul>
<p><strong>Key Considerations:</strong></p>
<ul>
<li>Learning rate tuning</li>
<li>Initial value selection (multiple starting points for local minima)</li>
<li>Convergence criteria</li>
</ul>
<h3 id="%F0%9F%93%89-loss-functions">📉 Loss Functions</h3>
<p><strong>Common Loss Functions:</strong></p>
<ol>
<li>
<p><strong>Least Squares (Linear Regression):</strong></p>
<pre class="hljs"><code><div>J(θ;D) = Σ(f_θ(x^(i)) - y_i)²
</div></code></pre>
</li>
<li>
<p><strong>Binary Cross Entropy (Logistic Regression):</strong></p>
<pre class="hljs"><code><div>BCE(w;D) = Σ[-y^(i)log(σ(w^T x^(i))) - (1-y^(i))log(1-σ(w^T x^(i)))]
</div></code></pre>
</li>
<li>
<p><strong>Hinge Loss (SVMs):</strong></p>
<pre class="hljs"><code><div>L_hinge = (1/2)||w||² + (C/n)Σmax{0; 1-y^(i)(w^T x^(i) + w_0)}
</div></code></pre>
</li>
</ol>
<h3 id="%F0%9F%94%92-constrained-optimization">🔒 Constrained Optimization</h3>
<p><strong>Core Concept:</strong> Optimize objectives subject to constraints using Lagrangian methods.</p>
<p><strong>Key Components:</strong></p>
<ul>
<li><strong>Lagrangian:</strong> Incorporate constraints into objective function</li>
<li><strong>Lagrange Multipliers:</strong> Handle equality and inequality constraints</li>
<li><strong>Dual Problem:</strong> Alternative formulation often easier to solve</li>
<li><strong>Complementary Slackness:</strong> Relationship between primal and dual solutions</li>
</ul>
<p><strong>SVM Example:</strong></p>
<ul>
<li><strong>Primal:</strong> Minimize margin subject to classification constraints</li>
<li><strong>Dual:</strong> Maximize margin in terms of support vectors (enables kernel trick)</li>
</ul>
<hr>
<h2 id="%F0%9F%9B%A0%EF%B8%8F-summary-table-supervised-learning-methods">🛠️ Summary Table: Supervised Learning Methods</h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Feature Type</th>
<th>Key Concepts</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>K-NN</strong></td>
<td>Mostly numerical</td>
<td>Distance metrics, lazy learning</td>
</tr>
<tr>
<td><strong>Polynomial Regression</strong></td>
<td>Numerical</td>
<td>Feature transformation, overfitting control</td>
</tr>
<tr>
<td><strong>Decision Trees</strong></td>
<td>Any type</td>
<td>Entropy, Gini, information gain, pruning</td>
</tr>
<tr>
<td><strong>Bayesian Classifiers</strong></td>
<td>Mixed</td>
<td>Probability distributions, priors, posteriors</td>
</tr>
<tr>
<td><strong>Logistic Regression</strong></td>
<td>Numerical</td>
<td>Log-odds, BCE loss, probabilistic output</td>
</tr>
<tr>
<td><strong>SVM</strong></td>
<td>Numerical</td>
<td>Max-margins, kernels, slack variables</td>
</tr>
</tbody>
</table>
<hr>
<div align="center">
</div><h3 id="%F0%9F%8E%93-course-completion">🎓 Course Completion</h3>
<p><em>This summary covers the comprehensive machine learning curriculum from CS3141 at Reichman University. Each method has its strengths and optimal use cases - the key is understanding when and how to apply them effectively.</em></p>
<p><strong>Happy (and quiet) Summer! 🏖️</strong></p>
<hr>
<p><img src="https://img.shields.io/badge/GitHub-Repository-black?style=flat&amp;logo=github" alt="GitHub"><br>
<img src="https://img.shields.io/badge/Made%20with-Markdown-blue?style=flat&amp;logo=markdown" alt="Markdown"></p>

</body>

</html>