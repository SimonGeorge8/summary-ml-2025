# ğŸ¤– Machine Learning from Data - Course Summary

<div align="center">

![Machine Learning](https://img.shields.io/badge/Machine%20Learning-CS3141-blue?style=for-the-badge)
![University](https://img.shields.io/badge/Reichman%20University-2025-orange?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Complete-green?style=for-the-badge)

*A comprehensive overview of machine learning concepts covering supervised learning, unsupervised learning, and optimization techniques.*

**Instructors:** Prof. Ilan Gronau & Dr. Alon Kipnis

</div>

---

## ğŸ“š Table of Contents

- [ğŸ¯ Supervised Learning](#-supervised-learning)
  - [ğŸ” K-Nearest Neighbors](#-k-nearest-neighbors-k-nn)
  - [ğŸ“ˆ Polynomial Regression](#-polynomial-regression)
  - [ğŸŒ³ Decision Trees](#-decision-trees)
  - [ğŸ² Bayesian Classification](#-bayesian-classification)
  - [ğŸ§  Perceptron](#-perceptron)
  - [ğŸ“Š Logistic Regression](#-logistic-regression)
  - [âš¡ Support Vector Machines](#-support-vector-machines-svm)
  - [ğŸ“ Model Evaluation](#-model-evaluation)
- [ğŸ” Unsupervised Learning](#-unsupervised-learning)
  - [ğŸ“Š Data Distribution Learning](#-data-distribution-learning)
  - [ğŸ”— Clustering](#-clustering)
  - [ğŸ”„ Expectation Maximization](#-expectation-maximization)
  - [ğŸ“‰ Dimensionality Reduction](#-dimensionality-reduction)
- [âš™ï¸ Optimization](#ï¸-optimization)
  - [ğŸ¯ Gradient-Based Methods](#-gradient-based-methods)
  - [ğŸ“‰ Loss Functions](#-loss-functions)
  - [ğŸ”’ Constrained Optimization](#-constrained-optimization)

---

## ğŸ¯ Supervised Learning

Supervised learning involves training models on labeled data to make predictions on new, unseen data. The training process uses input-output pairs to learn patterns and relationships.

### ğŸ” K-Nearest Neighbors (K-NN)

**Core Concept:** Make predictions based on the k nearest neighbors in the feature space.

- **For Regression:** Average the values of k nearest neighbors
- **For Classification:** Take majority vote among k nearest neighbors
- **Hyperparameter:** k (number of neighbors)

**Pros:**
- âœ… Good fit to training data with minimal assumptions
- âœ… Works well with non-linear patterns

**Cons:**
- âŒ Computationally inefficient for large datasets
- âŒ Struggles with high-dimensional spaces (curse of dimensionality)

### ğŸ“ˆ Polynomial Regression

**Core Concept:** Fit polynomial functions to data by transforming features into higher-degree terms.

- **Process:** Transform features using polynomial basis functions â†’ Solve linear regression
- **Hyperparameter:** k (maximum degree of polynomial)
- **Method:** Minimize sum-of-squares loss via gradient descent or pseudo-inverse

**Use Case:** When relationships between variables are non-linear but can be captured by polynomial terms.

### ğŸŒ³ Decision Trees

**Core Concept:** Create a tree-like model of decisions to classify data points.

- **Training:** Iterative node splitting using attributes that maximize impurity reduction
- **Key Metrics:** Entropy, Gini Impurity, Information Gain, Chi-squared test
- **Hyperparameters:** Max depth, max leaves, min samples per leaf, min impurity decrease

**Pros:**
- âœ… Excellent for categorical features
- âœ… No specific modeling assumptions
- âœ… Interpretable results

**Cons:**
- âŒ Can create very large, inefficient models
- âŒ Rectangular decision boundaries only
- âŒ Prone to overfitting

### ğŸ² Bayesian Classification

**Core Concept:** Use Bayes' theorem and probability distributions to classify data.

- **Training:** Fit class priors and class-conditional distributions via maximum likelihood
- **Prediction:** Minimize expected risk using posterior probabilities
- **Variants:** MAP (uniform cost), ML (uniform priors), NaÃ¯ve Bayes (feature independence)

**Formula:** `c(x) = argmin_j Î£ Ï€_l f_{X|Y=l}(x) Î»_{j,l}`

**Pros:**
- âœ… Easily scales to multiple classes
- âœ… Flexible decision boundaries
- âœ… Can incorporate different costs for classification errors

**Cons:**
- âŒ Challenging to learn distributions in high dimensions

### ğŸ§  Perceptron

**Core Concept:** Linear classifier that finds a separating hyperplane.

- **Capability:** Finds linear separation boundaries
- **Extension:** Non-linear boundaries via feature mapping or dual formulation with kernels
- **Status:** Not practical (SVM is the better alternative)

### ğŸ“Š Logistic Regression

**Core Concept:** Linear classifier optimized for probabilistic interpretation.

- **Method:** Minimize binary cross-entropy (BCE) loss
- **Decision Boundary:** Linear
- **Extensions:** Natural extension to multiple classes
- **Hyperparameters:** None (typically)

**Best For:** Cases with no pure linear separation, provides probability estimates for classifications.

### âš¡ Support Vector Machines (SVM)

**Core Concept:** Find the optimal separating hyperplane with maximum margin.

- **Objective:** Maximize margin between classes
- **Extensions:** Kernel functions for non-linear boundaries
- **Slack Variables:** Allow margin violations and classification errors
- **Loss Function:** Hinge loss
- **Hyperparameters:** Kernel function, kernel parameters, C (slack variable penalty)

**Pros:**
- âœ… The "go-to" method for classification
- âœ… Flexible decision boundaries via kernels
- âœ… Some interpretability through support vectors

**Cons:**
- âŒ Relatively computationally heavy to train

### ğŸ“ Model Evaluation

**Key Concepts:**
- **Bias-Variance Tradeoff:** Balance between underfitting and overfitting
- **Data Splitting:** Train â†’ Validate â†’ Test workflow
- **Metrics:** Confusion matrix, precision, recall, F1-score, ROC AUC

**Process:**
1. **Training Set:** Fit models
2. **Validation Set:** Select hyperparameters  
3. **Test Set:** Assess final generalization error

---

## ğŸ” Unsupervised Learning

Unsupervised learning finds patterns and structure in data without labeled examples, focusing on discovering hidden relationships and groupings.

### ğŸ“Š Data Distribution Learning

**Core Concept:** Learn the underlying probability distribution of data.

**Methods:**
- **Non-parametric:** Histogram smoothing techniques
- **Parametric:** Maximum likelihood estimation (typically log-likelihood)

**Common Distributions:**
- Binomial, Poisson, Exponential
- **Normal Distributions:** Univariate/Multivariate Gaussians (particularly useful)
- **Gaussian Mixture Models (GMMs)**

**Applications:** Bayesian classification, understanding data properties (mean, modes, variation)

### ğŸ”— Clustering

**Core Concept:** Partition data into "closely clustered" subsets of samples.

**Algorithms:**
- **K-means:** Minimize within-cluster spread for given number of clusters (k)
- **Hierarchical Clustering:** Build hierarchy represented by dendrogram

**Applications:** 
- Describe data structure
- Understand generative processes (e.g., evolutionary trees)
- Data exploration and segmentation

### ğŸ”„ Expectation Maximization

**Core Concept:** Specialized optimization for complex probabilistic models with hidden variables.

**Key Features:**
- âœ… Iterative algorithm without requiring learning rate tuning
- âœ… Useful when likelihood is simplified by introducing hidden variables
- âœ… Common for cluster labels and mixture model components

**Applications:** GMM estimation, clustering with probabilistic assignments

### ğŸ“‰ Dimensionality Reduction

**Methods:** PCA (Principal Component Analysis) + LDA (Linear Discriminant Analysis)

*Note: Not covered in exam for this course year*

---

## âš™ï¸ Optimization

Optimization techniques are fundamental to training machine learning models, involving the minimization or maximization of objective functions.

### ğŸ¯ Gradient-Based Methods

**Core Concept:** Use gradient information to iteratively find optimal parameters.

**Analytical Solutions:**
- Pseudoinverse method for linear regression
- Maximum likelihood estimates for many distributions

**Gradient Descent Variants:**
- **Standard GD:** Full dataset gradient computation
- **Stochastic GD:** Use batches for efficiency
- **Sub-gradient Descent:** Handle non-continuous gradient points (e.g., hinge loss)

**Key Considerations:**
- Learning rate tuning
- Initial value selection (multiple starting points for local minima)
- Convergence criteria

### ğŸ“‰ Loss Functions

**Common Loss Functions:**

1. **Least Squares (Linear Regression):**
   ```
   J(Î¸;D) = Î£(f_Î¸(x^(i)) - y_i)Â²
   ```

2. **Binary Cross Entropy (Logistic Regression):**
   ```
   BCE(w;D) = Î£[-y^(i)log(Ïƒ(w^T x^(i))) - (1-y^(i))log(1-Ïƒ(w^T x^(i)))]
   ```

3. **Hinge Loss (SVMs):**
   ```
   L_hinge = (1/2)||w||Â² + (C/n)Î£max{0; 1-y^(i)(w^T x^(i) + w_0)}
   ```

### ğŸ”’ Constrained Optimization

**Core Concept:** Optimize objectives subject to constraints using Lagrangian methods.

**Key Components:**
- **Lagrangian:** Incorporate constraints into objective function
- **Lagrange Multipliers:** Handle equality and inequality constraints
- **Dual Problem:** Alternative formulation often easier to solve
- **Complementary Slackness:** Relationship between primal and dual solutions

**SVM Example:**
- **Primal:** Minimize margin subject to classification constraints
- **Dual:** Maximize margin in terms of support vectors (enables kernel trick)

---

## ğŸ› ï¸ Summary Table: Supervised Learning Methods

| Method | Feature Type | Key Concepts |
|--------|-------------|--------------|
| **K-NN** | Mostly numerical | Distance metrics, lazy learning |
| **Polynomial Regression** | Numerical | Feature transformation, overfitting control |
| **Decision Trees** | Any type | Entropy, Gini, information gain, pruning |
| **Bayesian Classifiers** | Mixed | Probability distributions, priors, posteriors |
| **Logistic Regression** | Numerical | Log-odds, BCE loss, probabilistic output |
| **SVM** | Numerical | Max-margins, kernels, slack variables |

---

<div align="center">

### ğŸ“ Course Completion

*This summary covers the comprehensive machine learning curriculum from CS3141 at Reichman University. Each method has its strengths and optimal use cases - the key is understanding when and how to apply them effectively.*

**Happy (and quiet) Summer! ğŸ–ï¸**

---

![GitHub](https://img.shields.io/badge/GitHub-Repository-black?style=flat&logo=github)
![Markdown](https://img.shields.io/badge/Made%20with-Markdown-blue?style=flat&logo=markdown)

</div>